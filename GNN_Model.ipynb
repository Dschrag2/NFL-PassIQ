{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d565ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4037c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Plays: 100%|██████████| 24863/24863 [08:30<00:00, 48.73play/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.4656\n",
      "Epoch 2/100, Loss: 0.7786\n",
      "Epoch 3/100, Loss: 0.7574\n",
      "Epoch 4/100, Loss: 0.7443\n",
      "Epoch 5/100, Loss: 0.7306\n",
      "Epoch 6/100, Loss: 0.7076\n",
      "Epoch 7/100, Loss: 0.6946\n",
      "Epoch 8/100, Loss: 0.6969\n",
      "Epoch 9/100, Loss: 0.6796\n",
      "Epoch 10/100, Loss: 0.6625\n",
      "Epoch 11/100, Loss: 0.6574\n",
      "Epoch 12/100, Loss: 0.6465\n",
      "Epoch 13/100, Loss: 0.6421\n",
      "Epoch 14/100, Loss: 0.6308\n",
      "Epoch 15/100, Loss: 0.6228\n",
      "Epoch 16/100, Loss: 0.6173\n",
      "Epoch 17/100, Loss: 0.6074\n",
      "Epoch 18/100, Loss: 0.6040\n",
      "Epoch 19/100, Loss: 0.5978\n",
      "Epoch 20/100, Loss: 0.5924\n",
      "Epoch 21/100, Loss: 0.5859\n",
      "Epoch 22/100, Loss: 0.5789\n",
      "Epoch 23/100, Loss: 0.5762\n",
      "Epoch 24/100, Loss: 0.5700\n",
      "Epoch 25/100, Loss: 0.5641\n",
      "Epoch 26/100, Loss: 0.5606\n",
      "Epoch 27/100, Loss: 0.5588\n",
      "Epoch 28/100, Loss: 0.5528\n",
      "Epoch 29/100, Loss: 0.5433\n",
      "Epoch 30/100, Loss: 0.5402\n",
      "Epoch 31/100, Loss: 0.5322\n",
      "Epoch 32/100, Loss: 0.5263\n",
      "Epoch 33/100, Loss: 0.5275\n",
      "Epoch 34/100, Loss: 0.5168\n",
      "Epoch 35/100, Loss: 0.5144\n",
      "Epoch 36/100, Loss: 0.5035\n",
      "Epoch 37/100, Loss: 0.4956\n",
      "Epoch 38/100, Loss: 0.4905\n",
      "Epoch 39/100, Loss: 0.4918\n",
      "Epoch 40/100, Loss: 0.4847\n",
      "Epoch 41/100, Loss: 0.4793\n",
      "Epoch 42/100, Loss: 0.4730\n",
      "Epoch 43/100, Loss: 0.4669\n",
      "Epoch 44/100, Loss: 0.4653\n",
      "Epoch 45/100, Loss: 0.4633\n",
      "Epoch 46/100, Loss: 0.4564\n",
      "Epoch 47/100, Loss: 0.4517\n",
      "Epoch 48/100, Loss: 0.4466\n",
      "Epoch 49/100, Loss: 0.4406\n",
      "Epoch 50/100, Loss: 0.4436\n",
      "Epoch 51/100, Loss: 0.4387\n",
      "Epoch 52/100, Loss: 0.4285\n",
      "Epoch 53/100, Loss: 0.4282\n",
      "Epoch 54/100, Loss: 0.4260\n",
      "Epoch 55/100, Loss: 0.4194\n",
      "Epoch 56/100, Loss: 0.4145\n",
      "Epoch 57/100, Loss: 0.4173\n",
      "Epoch 58/100, Loss: 0.4118\n",
      "Epoch 59/100, Loss: 0.4076\n",
      "Epoch 60/100, Loss: 0.4052\n",
      "Epoch 61/100, Loss: 0.4024\n",
      "Epoch 62/100, Loss: 0.3951\n",
      "Epoch 63/100, Loss: 0.4069\n",
      "Epoch 64/100, Loss: 0.3866\n",
      "Epoch 65/100, Loss: 0.3856\n",
      "Epoch 66/100, Loss: 0.3865\n",
      "Epoch 67/100, Loss: 0.3782\n",
      "Epoch 68/100, Loss: 0.3864\n",
      "Epoch 69/100, Loss: 0.3828\n",
      "Epoch 70/100, Loss: 0.3745\n",
      "Epoch 71/100, Loss: 0.3720\n",
      "Epoch 72/100, Loss: 0.3701\n",
      "Epoch 73/100, Loss: 0.3673\n",
      "Epoch 74/100, Loss: 0.3614\n",
      "Epoch 75/100, Loss: 0.3682\n",
      "Epoch 76/100, Loss: 0.3641\n",
      "Epoch 77/100, Loss: 0.3609\n",
      "Epoch 78/100, Loss: 0.3684\n",
      "Epoch 79/100, Loss: 0.3567\n",
      "Epoch 80/100, Loss: 0.3596\n",
      "Epoch 81/100, Loss: 0.3500\n",
      "Epoch 82/100, Loss: 0.3524\n",
      "Epoch 83/100, Loss: 0.3478\n",
      "Epoch 84/100, Loss: 0.3521\n",
      "Epoch 85/100, Loss: 0.3475\n",
      "Epoch 86/100, Loss: 0.3446\n",
      "Epoch 87/100, Loss: 0.3416\n",
      "Epoch 88/100, Loss: 0.3424\n",
      "Epoch 89/100, Loss: 0.3463\n",
      "Epoch 90/100, Loss: 0.3388\n",
      "Epoch 91/100, Loss: 0.3440\n",
      "Epoch 92/100, Loss: 0.3353\n",
      "Epoch 93/100, Loss: 0.3466\n",
      "Epoch 94/100, Loss: 0.3366\n",
      "Epoch 95/100, Loss: 0.3367\n",
      "Epoch 96/100, Loss: 0.3298\n",
      "Epoch 97/100, Loss: 0.3289\n",
      "Epoch 98/100, Loss: 0.3374\n",
      "Epoch 99/100, Loss: 0.3289\n",
      "Epoch 100/100, Loss: 0.3275\n",
      "This shit done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plays_data = pd.read_csv(\"data/plays.csv\")\n",
    "player_data = pd.read_csv(\"data/players.csv\")\n",
    "player_play_data = pd.read_csv(\"data/player_play.csv\")\n",
    "game_data = pd.read_csv(\"data/games.csv\")\n",
    "\n",
    "passing_plays = plays_data[plays_data[\"passResult\"].isin([\"C\", \"I\", \"IN\"])]\n",
    "\n",
    "weight_params = {\n",
    "    'dis': 1.0,\n",
    "    's': 0.3,\n",
    "    'a': 0.3,\n",
    "    'dir': 0.12,\n",
    "    'o': 0.12\n",
    "}\n",
    "\n",
    "def compute_edge_weight(player1, player2, weights):\n",
    "    # print(player1, player2)\n",
    "    \"\"\"\n",
    "    Calculate a combined edge weight between two players.\n",
    "    \n",
    "    Parameters:\n",
    "    - player1, player2: Pandas Series or dict-like objects with keys \n",
    "      'x', 'y', 'speed', 'acceleration', 'dir', 'o'.\n",
    "    - weights: Dictionary with keys 'distance', 'speed', 'acc', 'dir', 'ori'\n",
    "      representing the coefficient for each term.\n",
    "      \n",
    "    Returns:\n",
    "    - A scalar weight for the edge.\n",
    "    \"\"\"\n",
    "    # Calculate Euclidean distance between positions.\n",
    "    pos_diff = np.sqrt((player1['x'] - player2['x'])**2 + (player1['y'] - player2['y'])**2)\n",
    "    \n",
    "    # Compute absolute differences for speed and acceleration.\n",
    "    speed_diff = abs(player1['s'] - player2['s'])\n",
    "    acc_diff = abs(player1['a'] - player2['a'])\n",
    "    \n",
    "    # Compute the angle differences (ensure angles are treated correctly).\n",
    "    dir_diff = angle_difference(player1['dir'], player2['dir'])\n",
    "    ori_diff = angle_difference(player1['o'], player2['o'])\n",
    "    \n",
    "    # Combine using the provided weights.\n",
    "    edge_weight = (weights['dis'] * pos_diff +\n",
    "                   weights['s'] * speed_diff +\n",
    "                   weights['a'] * acc_diff +\n",
    "                   weights['dir'] * dir_diff +\n",
    "                   weights['o'] * ori_diff)\n",
    "    return edge_weight\n",
    "\n",
    "\n",
    "def angle_difference(angle1, angle2):\n",
    "    \"\"\"\n",
    "    Compute the minimal absolute difference between two angles (in degrees)\n",
    "    taking into account wrap-around at 360 degrees.\n",
    "    \"\"\"\n",
    "    diff = abs(angle1 - angle2) % 360\n",
    "    if diff > 180:\n",
    "        diff = 360 - diff\n",
    "    return diff\n",
    "\n",
    "def adjust_position(row, game, play): # game and play are rows of the game and play dataframes\n",
    "\n",
    "    row = row.copy()\n",
    "\n",
    "    if play[\"possessionTeam\"].iloc[0] == game[\"homeTeamAbbr\"].iloc[0]:\n",
    "        if play[\"yardlineSide\"].iloc[0] == game[\"homeTeamAbbr\"].iloc[0]:\n",
    "            absYard = play[\"yardlineNumber\"].iloc[0] + 10\n",
    "        elif play[\"yardlineSide\"].iloc[0] == game[\"visitorTeamAbbr\"].iloc[0]:\n",
    "            absYard = 110 - play[\"yardlineNumber\"].iloc[0]\n",
    "        else:\n",
    "            absYard = 60\n",
    "\n",
    "        row.loc[\"x\"] -= absYard\n",
    "        row.loc[\"o\"] += 90\n",
    "        row.loc[\"dir\"] += 90\n",
    "        if row[\"o\"] > 360: row[\"o\"] -= 360\n",
    "        if row[\"dir\"] > 360: row[\"dir\"] -= 360\n",
    "\n",
    "    elif play[\"possessionTeam\"].iloc[0] == game[\"visitorTeamAbbr\"].iloc[0]:\n",
    "        if play[\"yardlineSide\"].iloc[0] == game[\"visitorTeamAbbr\"].iloc[0]:\n",
    "            absYard = 110 - play[\"yardlineNumber\"].iloc[0]\n",
    "        elif play[\"yardlineSide\"].iloc[0] == game[\"homeTeamAbbr\"].iloc[0]:\n",
    "            absYard = play[\"yardlineNumber\"].iloc[0] + 10\n",
    "        else:\n",
    "            absYard = 60\n",
    "\n",
    "        row.loc[\"x\"] = absYard - row[\"x\"]\n",
    "        row.loc[\"y\"] = 53.3 - row[\"y\"]\n",
    "        row.loc[\"o\"] += 270\n",
    "        row.loc[\"dir\"] += 270\n",
    "        if row[\"o\"] > 360: row[\"o\"] -= 360\n",
    "        if row[\"dir\"] > 360: row[\"dir\"] -= 360\n",
    "\n",
    "    else:\n",
    "        print(\"SOMETHING WENT WRONG\")\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def get_targeted_receiver(Recs, nflIDs):\n",
    "    \n",
    "    for WR in Recs.itertuples(index=True):\n",
    "        if (WR.wasTargettedReceiver):\n",
    "            return np.where(nflIDs == WR.nflId)[0][0]\n",
    "\n",
    "def process_play(row):\n",
    "    # print(row)\n",
    "    currPlayId = row[\"playId\"]\n",
    "    currGameId = row[\"gameId\"]\n",
    "\n",
    "    curr_game = game_data[game_data[\"gameId\"] == row[\"gameId\"]]\n",
    "    curr_play = plays_data[(plays_data[\"gameId\"] == row[\"gameId\"]) & (plays_data[\"playId\"] == row[\"playId\"])]\n",
    "\n",
    "    row = adjust_position(row, curr_game, curr_play)\n",
    "    # print(\"-----------------\")\n",
    "\n",
    "    Recs = player_play_data[(player_play_data[\"gameId\"] == row[\"gameId\"]) & (player_play_data[\"playId\"] == row[\"playId\"]) & (player_play_data[\"wasRunningRoute\"] == True)]\n",
    "    Defs = player_play_data[(player_play_data[\"gameId\"] == row[\"gameId\"]) & (player_play_data[\"playId\"] == row[\"playId\"]) & ~pd.isna(player_play_data[\"pff_defensiveCoverageAssignment\"])]\n",
    "\n",
    "    Recs = week_data_passed[(week_data_passed[\"gameId\"] == row[\"gameId\"]) & (week_data_passed[\"playId\"] == row[\"playId\"]) & (week_data_passed[\"nflId\"].isin(Recs[\"nflId\"]))].copy()\n",
    "    Defs = week_data_passed[(week_data_passed[\"gameId\"] == row[\"gameId\"]) & (week_data_passed[\"playId\"] == row[\"playId\"]) & (week_data_passed[\"nflId\"].isin(Defs[\"nflId\"]))].copy()\n",
    "\n",
    "    Recs = Recs.reset_index(drop=True)\n",
    "    Defs = Defs.reset_index(drop=True)\n",
    "\n",
    "    R_nodes = np.arange(len(Recs))\n",
    "    D_nodes = np.arange(len(Recs), len(Defs) + len(Recs))\n",
    "\n",
    "    play_data = {} # for analyzing all recs/defenders\n",
    "    play_data[\"edges\"] = np.vstack([np.repeat(R_nodes, len(Defs)), np.tile(D_nodes, len(Recs))])\n",
    "\n",
    "    edge_index_list = []\n",
    "    edge_weight_list = []\n",
    "\n",
    "    for rec_idx, rec in Recs.iterrows():\n",
    "        for def_idx, defe in Defs.iterrows():\n",
    "            # Append the edge: receiver -> defender.\n",
    "            edge_index_list.append([rec_idx, def_idx + len(Recs)])\n",
    "            \n",
    "            # Compute the edge weight using the custom function.\n",
    "            weight_value = compute_edge_weight(rec, defe, weight_params)\n",
    "            edge_weight_list.append([weight_value])  # Wrapped in a list for proper shape\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    currPlay = plays_data[(plays_data[\"playId\"] == currPlayId) & (plays_data[\"gameId\"] == currGameId)]\n",
    "    # print(currPlay)\n",
    "\n",
    "    return edge_index_list, edge_weight_list, Recs, Defs, currPlay\n",
    "\n",
    "\n",
    "\n",
    "week_data = pd.read_csv(\"data/tracking_week_1.csv\")\n",
    "week_data_passed = week_data[week_data[\"event\"].isin([\"pass_forward\", \"pass_shovel\"])]\n",
    "week_data_arrived = week_data[week_data[\"event\"].isin([\"pass_arrived\"])]\n",
    "# print(len(week_data_arrived))\n",
    "# print(week_data_arrived.iloc[0])\n",
    "\n",
    "# arrived_play_keys = set(zip(week_data_arrived[\"gameId\"], week_data_arrived[\"playId\"]))\n",
    "# print(len(arrived_play_keys))\n",
    "\n",
    "a=0\n",
    "data_list = []\n",
    "\n",
    "if os.path.exists(\"processed_plays.pt\"):\n",
    "    print(\"Loading cached processed plays...\")\n",
    "    data_list = torch.load(\"processed_plays.pt\")\n",
    "else:\n",
    "    for i, row in tqdm(week_data_passed.iterrows(), total=len(week_data_passed), desc=\"Processing Plays\", unit=\"play\"):\n",
    "        # print(i, row)\n",
    "        edge_index_list, edge_weight_list, Recs, Defs, currPlay = process_play(row)\n",
    "        if Recs.empty or Defs.empty:\n",
    "                continue\n",
    "        rec_features = Recs[['x', 'y', 's', 'a', 'dir', 'o']].to_numpy()\n",
    "        def_features = Defs[['x', 'y', 's', 'a', 'dir', 'o']].to_numpy()\n",
    "        node_features = np.vstack([rec_features, def_features])\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        if len(edge_index_list) == 0:\n",
    "            continue\n",
    "\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous() \n",
    "        edge_attr = torch.tensor(edge_weight_list, dtype=torch.float) \n",
    "\n",
    "        pass_result = currPlay[\"passResult\"].iloc[0]\n",
    "        if pass_result == \"S\":\n",
    "            continue\n",
    "        # print(\"Pass Result:\", pass_result)\n",
    "        label_mapping = {\"C\": 0, \"I\": 1, \"IN\": 2}\n",
    "        y = torch.tensor(label_mapping[pass_result], dtype=torch.long)\n",
    "\n",
    "        data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        data_obj.batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "        data_list.append(data_obj)\n",
    "\n",
    "torch.save(data_list, \"processed_plays.pt\")\n",
    "\n",
    "\n",
    "\n",
    "class NFLGraphClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(NFLGraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # A fully connected layer to map the global pooled vector to class logits.\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch if hasattr(data, 'batch') else None\n",
    "        \n",
    "        # First GCN layer with ReLU activation.\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer.\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Global pooling: aggregate node embeddings to form a graph representation.\n",
    "        # If working with a single graph, you can supply a batch tensor of zeros.\n",
    "        if batch is None:  \n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Final classification layer.\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = NFLGraphClassifier(in_channels=6, hidden_channels=32, num_classes=3)\n",
    "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: obtain logits\n",
    "    out = model(data)\n",
    "    # data.y contains the ground truth label for the graph.\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        loss = train(model, batch)\n",
    "        total_loss += loss  \n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"This shit done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# STEP 1: Constructing the Graph Data\n",
    "# -----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### NOT WORKING BELOW HERE YET ######### RETURN IS INTENTIONAL TO STOP EXECUTION\n",
    "\n",
    "\n",
    "\n",
    "# Example: Suppose each play involves 11 players.\n",
    "# For simplicity, here we'll generate random (x, y) positions for each player.\n",
    "\n",
    "\n",
    "# Create graph edges:\n",
    "# For the NFL simulation, you might either:\n",
    "#   - Connect all players (fully connected), or\n",
    "#   - Connect only nearby players using k-Nearest Neighbors (kNN)\n",
    "# For demonstration, we’ll construct a fully connected graph (excluding self-loops).\n",
    "\n",
    "# edge_index_list = []\n",
    "# edge_attr_list = []\n",
    "\n",
    "# for i in range(num_players):\n",
    "#     for j in range(num_players):\n",
    "#         if i != j:\n",
    "#             edge_index_list.append([i, j])\n",
    "#             # Compute Euclidean distance as separation between players i and j.\n",
    "#             distance = torch.norm(player_positions[i] - player_positions[j], p=2)\n",
    "#             edge_attr_list.append([distance])  # wrapped in a list to create a 2D tensor later\n",
    "\n",
    "# # Convert lists to tensors.\n",
    "# edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()  # shape [2, num_edges]\n",
    "# edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)  # shape [num_edges, 1]\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # STEP 2: Define the Ground Truth Label\n",
    "# # -----------------------------------------\n",
    "# # Map the pass outcomes to integer labels.\n",
    "# # For example:\n",
    "# #    0 => complete, 1 => incomplete, 2 => interception\n",
    "# pass_outcome = \"complete\"  # This would come from your dataset in practice.\n",
    "# label_mapping = {'complete': 0, 'incomplete': 1, 'interception': 2}\n",
    "# # Graph-level label as a tensor\n",
    "# graph_label = torch.tensor(label_mapping[pass_outcome], dtype=torch.long)\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # STEP 3: Create the PyTorch Geometric Data Object\n",
    "# # -----------------------------------------\n",
    "# # Note: In a realistic scenario, you might also have additional node features.\n",
    "# data = Data(\n",
    "#     x=player_positions,     # Node features (e.g., player positions)\n",
    "#     edge_index=edge_index,  # Graph connectivity\n",
    "#     edge_attr=edge_attr,    # Edge weights (separation between players)\n",
    "#     y=graph_label         # Graph-level label (pass outcome)\n",
    "# )\n",
    "\n",
    "# print(\"Constructed Graph Data:\\n\", data)\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # STEP 4: Define a GNN Model for Graph-Level Classification\n",
    "# # -----------------------------------------\n",
    "\n",
    "# class NFLGraphClassifier(nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "#         super(NFLGraphClassifier, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         # A fully connected layer to map the global pooled vector to class logits.\n",
    "#         self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "    \n",
    "#     def forward(self, data):\n",
    "#         x, edge_index, batch = data.x, data.edge_index, data.batch if hasattr(data, 'batch') else None\n",
    "        \n",
    "#         # First GCN layer with ReLU activation.\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         # Second GCN layer.\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "#         # Global pooling: aggregate node embeddings to form a graph representation.\n",
    "#         # If working with a single graph, you can supply a batch tensor of zeros.\n",
    "#         if batch is None:  \n",
    "#             batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "#         x = global_mean_pool(x, batch)\n",
    "        \n",
    "#         # Final classification layer.\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # -----------------------------------------\n",
    "# # STEP 5: Example Training Setup\n",
    "# # -----------------------------------------\n",
    "\n",
    "# # Hyperparameters for our model:\n",
    "# in_channels = player_positions.size(1)  # Number of node features (here 2: [x, y])\n",
    "# hidden_channels = 32\n",
    "# num_classes = 3  # complete, incomplete, interception\n",
    "\n",
    "# # Initialize our GNN model.\n",
    "# model = NFLGraphClassifier(in_channels, hidden_channels, num_classes)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = nn.CrossEntropyLoss()  # Expects raw logits and ground truth labels.\n",
    "\n",
    "# def train(model, data):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     # Forward pass: obtain logits\n",
    "#     out = model(data)\n",
    "#     # data.y contains the ground truth label for the graph.\n",
    "#     loss = criterion(out, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "# # To enable global pooling for a single graph, we assign a dummy batch index to all nodes.\n",
    "# data.batch = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "\n",
    "# # Training loop (one iteration for demo purposes).\n",
    "# loss_val = train(model, data)\n",
    "# print(\"Training loss:\", loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b44fc7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing unseen play...\n",
      "Predicted outcome: Complete\n",
      "Class probabilities: [0.8776790499687195, 0.1223168894648552, 4.081518000020878e-06]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming all your previous functions (process_play, adjust_position, compute_edge_weight, etc.)\n",
    "# are defined and available in the current namespace.\n",
    "\n",
    "# For example, here is a stub of your trained model (make sure to load your trained state_dict if saved):\n",
    "class NFLGraphClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(NFLGraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch if hasattr(data, 'batch') else None\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Function to process a single unseen play and return a PyTorch Geometric Data object.\n",
    "def process_unseen_play(play_row):\n",
    "    \"\"\"\n",
    "    Given a row for an unseen play, processes the play (adjust positions, create edge indices,\n",
    "    node features, and edge attributes) and returns a PyTorch Geometric Data object.\n",
    "    \"\"\"\n",
    "    currPlayId = play_row[\"playId\"]\n",
    "    currGameId = play_row[\"gameId\"]\n",
    "\n",
    "    # Select game and play information from your dataset.\n",
    "    curr_game = game_data[game_data[\"gameId\"] == play_row[\"gameId\"]]\n",
    "    curr_play = plays_data[(plays_data[\"gameId\"] == play_row[\"gameId\"]) & (plays_data[\"playId\"] == play_row[\"playId\"])]\n",
    "\n",
    "    play_row = adjust_position(play_row, curr_game, curr_play)\n",
    "\n",
    "    # Extract receivers and defenders based on your logic.\n",
    "    Recs = player_play_data[\n",
    "        (player_play_data[\"gameId\"] == play_row[\"gameId\"]) &\n",
    "        (player_play_data[\"playId\"] == play_row[\"playId\"]) &\n",
    "        (player_play_data[\"wasRunningRoute\"] == True)\n",
    "    ]\n",
    "    Defs = player_play_data[\n",
    "        (player_play_data[\"gameId\"] == play_row[\"gameId\"]) &\n",
    "        (player_play_data[\"playId\"] == play_row[\"playId\"]) &\n",
    "        ~pd.isna(player_play_data[\"pff_defensiveCoverageAssignment\"])\n",
    "    ]\n",
    "\n",
    "    # Use tracking data to refine positions for the play.\n",
    "    Recs = week_data_passed[\n",
    "        (week_data_passed[\"gameId\"] == play_row[\"gameId\"]) &\n",
    "        (week_data_passed[\"playId\"] == play_row[\"playId\"]) &\n",
    "        (week_data_passed[\"nflId\"].isin(Recs[\"nflId\"]))\n",
    "    ].copy()\n",
    "    Defs = week_data_passed[\n",
    "        (week_data_passed[\"gameId\"] == play_row[\"gameId\"]) &\n",
    "        (week_data_passed[\"playId\"] == play_row[\"playId\"]) &\n",
    "        (week_data_passed[\"nflId\"].isin(Defs[\"nflId\"]))\n",
    "    ].copy()\n",
    "\n",
    "    # Reset indices to ensure proper node indexing.\n",
    "    Recs = Recs.reset_index(drop=True)\n",
    "    Defs = Defs.reset_index(drop=True)\n",
    "\n",
    "    # Create edge indices and compute edge attributes.\n",
    "    edge_index_list = []\n",
    "    edge_weight_list = []\n",
    "    for rec_idx, rec in Recs.iterrows():\n",
    "        for def_idx, defe in Defs.iterrows():\n",
    "            edge_index_list.append([rec_idx, def_idx + len(Recs)])\n",
    "            weight_value = compute_edge_weight(rec, defe, weight_params)\n",
    "            edge_weight_list.append([weight_value])\n",
    "    \n",
    "    # Build node features by stacking receiver and defender features.\n",
    "    rec_features = Recs[['x', 'y', 's', 'a', 'dir', 'o']].to_numpy()\n",
    "    def_features = Defs[['x', 'y', 's', 'a', 'dir', 'o']].to_numpy()\n",
    "    node_features = np.vstack([rec_features, def_features])\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    # Build edge tensors.\n",
    "    if len(edge_index_list) == 0:\n",
    "        return None  # or handle cases with no valid edge\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_weight_list, dtype=torch.float)\n",
    "\n",
    "    # Create the graph-level Data object.\n",
    "    # Here we assume that the pass outcome is unknown, so set a dummy label; prediction will replace it.\n",
    "    y = torch.tensor([-1], dtype=torch.long)  # dummy label\n",
    "    data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    data_obj.batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "    return data_obj\n",
    "\n",
    "# Function to run prediction on one play.\n",
    "def predict_play(play_row, model):\n",
    "    data_obj = process_unseen_play(play_row)\n",
    "    if data_obj is None:\n",
    "        print(\"No valid graph for this play.\")\n",
    "        return None\n",
    "    model.eval()  # Set model to evaluation mode.\n",
    "    with torch.no_grad():\n",
    "        logits = model(data_obj)  # Forward pass.\n",
    "        # Apply softmax to obtain class probabilities.\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    # Map the predicted class to the pass outcome label.\n",
    "    label_mapping = {0: \"Complete\", 1: \"Incomplete\", 2: \"Interception\"}\n",
    "    prediction = label_mapping.get(predicted_class, \"Unknown\")\n",
    "    return prediction, probs.squeeze().tolist()\n",
    "\n",
    "# -------------------------\n",
    "# Example usage:\n",
    "# Assume you have an unseen play (a row from your tracking/play dataset).\n",
    "# You could extract it from a CSV or use one from week_data_arrived.\n",
    "# For demonstration, we'll take the first row of week_data_arrived:\n",
    "randN = np.random.randint(1, 10000)\n",
    "unseen_play = week_data_arrived.iloc[randN]\n",
    "print(\"Processing unseen play...\")\n",
    "prediction, prob_scores = predict_play(unseen_play, model)\n",
    "print(\"Predicted outcome:\", prediction)\n",
    "print(\"Class probabilities:\", prob_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
